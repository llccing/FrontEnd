import{_ as t,c as o,a,o as r}from"./app-Dbw06csz.js";const s={};function n(i,e){return r(),o("div",null,[...e[0]||(e[0]=[a("<p>A company wants to use a large language model （LLM） on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt. Which adjustment to an inference parameter should the company make to meet these requirements? A.Decrease the temperature value. B.Increase the temperature value C.Decrease the length of output tokens. D.Increase the maximum generation length.</p><p>The correct answer is A. Decrease the temperature value.</p><p>Here is the detailed explanation for the correct answer and why the other options are incorrect.</p><p>Core Concept: Temperature in LLMs In Large Language Models (LLMs), Temperature is a configuration setting that controls the randomness of the model&#39;s output. Low Temperature (0.1 - 0.3): Makes the model more &quot;deterministic.&quot; It will consistently choose the most probable next word, leading to stable, repetitive, and factual results. High Temperature (0.7 - 1.0+): Makes the model more &quot;creative&quot; or random. It explores less likely word choices, leading to diverse and varied outputs.</p><p>Analysis of Options</p><p>A. Decrease the temperature value (Correct) Reason: For tasks like sentiment analysis, you want a specific, factual classification (e.g., &quot;Positive,&quot; &quot;Negative,&quot; or &quot;Neutral&quot;). By decreasing the temperature, you tell the model to be less &quot;creative&quot; and more predictable. This ensures that if you send the same text for analysis multiple times, the model is highly likely to return the exact same sentiment label every time.</p><p>B. Increase the temperature value Reason: Increasing temperature introduces more randomness. If you increased the temperature for sentiment analysis, the model might call a review &quot;Positive&quot; one time, &quot;Upbeat&quot; the next time, and &quot;Satisfactory&quot; the third time. This violates the company&#39;s requirement for consistency.</p><p>C. Decrease the length of output tokens Reason: This parameter (often called maxTokenCount or Max Length) simply limits how many words/tokens the model can generate before it stops. While sentiment analysis usually requires a short output, decreasing the length doesn&#39;t affect the consistency or logic of the choice; it only prevents the model from talking too much.</p><p>D. Increase the maximum generation length Reason: Similar to Option C, this just allows the model to write longer responses. It has no impact on whether the model chooses the same sentiment for the same prompt. In fact, for sentiment analysis, you generally want a very short generation length to save on costs and processing time.</p><p>Summary for the Exam When you see the word &quot;Consistent,&quot; &quot;Deterministic,&quot; or &quot;Reproducible&quot; in an Amazon Bedrock or AI exam question, the answer will almost always be Decreasing Temperature.</p><p>If the question asks for &quot;Creative,&quot; &quot;Human-like,&quot; or &quot;Varied&quot; responses, the answer is Increasing Temperature.</p>",11)])])}const l=t(s,[["render",n]]),u=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/065-temperatrue-high-low.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/065-temperatrue-high-low.md"}');export{l as comp,u as data};
