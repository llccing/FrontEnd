import{_ as e,c as t,a as s,o as i}from"./app-DxUSmTbc.js";const n={};function c(o,a){return i(),t("div",null,[...a[0]||(a[0]=[s('<h2 id="aws-database-caching" tabindex="-1"><a class="header-anchor" href="#aws-database-caching"><span>AWS Database Caching</span></a></h2><p>The speed and throughput of your database can be the most impactful factor for overall application performance.</p><p>https://aws.amazon.com/caching/database-caching/</p><p>In-memory data caching can be one of the most effective strategies to improve your overall application performance and to reduce your database costs.</p><p>Caching can be applied to any type of database including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved.</p><p>Below you will find some of the caching strategies and implementation approaches that can be taken to address the limitations and challenges associated with disk-based databases.</p><h3 id="types-of-caching" tabindex="-1"><a class="header-anchor" href="#types-of-caching"><span>Types of Caching</span></a></h3><h4 id="database-integration-caches" tabindex="-1"><a class="header-anchor" href="#database-integration-caches"><span>Database Integration Caches</span></a></h4><p>Database Integrated Caches: Some databases such as Amazon Aurora offer an integrated cache that is managed within the database engine and has built-in write-through capabilities. When the underlying data changes on the database table, the database updates its cache automatically, which is great. There is nothing within the application tier required to leverage this cache. Where integrated caches fall short is in their size and capabilities. Integrated caches are typically limited to the available memory allocated to the cache by the database instance and cannot be leveraged for other purposes, such as sharing data with other instances.</p><h4 id="local-caches" tabindex="-1"><a class="header-anchor" href="#local-caches"><span>Local Caches</span></a></h4><p>Local Caches: A local cache stores your frequently used data within your application. This not only speeds up your data retrieval but also removes network traffic associated with retrieving data, making data retrieval faster than other caching architectures. A major disadvantage is that among your applications, each node has its own resident cache working in a disconnected manner. The information stored within an individual cache node, whether its database cached data, web sessions or user shopping carts cannot be shared with other local caches. This creates challenges in a distributed environment where information sharing is critical to support scalable dynamic environments. And since most applications utilize multiple app servers, if each server has its own cache, coordinating the values across these becomes a major challenge.</p><p>In addition, when outages occur, the data in the local cache is lost and will need to be rehydrated effectively negating the cache. The majority of these cons are mitigated with remote caches. A remote cache (or “side cache”) is a separate instance (or multiple instances) dedicated for storing the cached data in-memory.</p><p>When network latency is of concern, a two-tier caching strategy can be applied that leverages a local and remote cache together. We won’t discuss this strategy in detail, but it is used typically used only when absolutely needed as it adds complexity. For most applications, the added network overhead associated with a remote cache is of little concern given that a request to it is generally fulfilled in sub-millisecond performance.</p><h4 id="remote-caches" tabindex="-1"><a class="header-anchor" href="#remote-caches"><span>Remote Caches</span></a></h4><p>Remote caches: Remote caches are stored on dedicated servers and typically built upon key/value NoSQL stores such as Redis and Memcached. They provide hundreds of thousands to up-to a million requests per second per cache node. Many solutions such as Amazon ElastiCache for Redis also provide the high availability needed for critical workloads.</p><p>Also, the average latency of a request to a remote cache is fulfilled in sub-millisecond latency, orders of magnitude faster than a disk-based database. At these speeds, local caches are seldom necessary. And since the remote cache works as a connected cluster that can be leveraged by all your disparate systems, they are ideal for distributed environments.</p>',16)])])}const h=e(n,[["render",c]]),d=JSON.parse('{"path":"/AWS/SAA/01Features/DatabaseCaching.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212945000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"510851a2e40013ba249eb2696f9e38f188ca533a","time":1768212945000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Update README.md"}]},"filePathRelative":"AWS/SAA/01Features/DatabaseCaching.md"}');export{h as comp,d as data};
