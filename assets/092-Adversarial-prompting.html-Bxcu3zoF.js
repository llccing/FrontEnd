import{_ as e,c as i,a as o,o as a}from"./app-Dbw06csz.js";const r={};function n(s,t){return a(),i("div",null,[...t[0]||(t[0]=[o("<p>Which prompting technique can protect against prompt injection attacks? A.Adversarial prompting B.Zero-shot prompting C.Least-to-most prompting D.Chain-of-thought prompting</p><p>The correct answer is A. Adversarial prompting.</p><p>While adversarial prompting is often used as a method to test or execute attacks, it is also a primary technique used by developers to protect against prompt injection. By using adversarial techniques during the training or fine-tuning phase, developers can teach a model to recognize and ignore malicious instructions.</p><p>Why Adversarial Prompting?</p><p>In the context of defense, this is often referred to as Adversarial Robustness Training. It involves:</p><ul><li>Red Teaming: Proactively trying to &quot;break&quot; the model with injection attacks to identify vulnerabilities.</li><li>Instructional Guardrails: Teaching the model to prioritize its system instructions over user-provided data that attempts to override them.</li><li>Input Sanitization: Crafting prompts that explicitly tell the model to treat user input as data only, not as commands.</li></ul><p>Why the other options are incorrect:</p><p>Technique,Description,Why it doesn&#39;t protect against injection Zero-shot,Asking a model to perform a task without any examples.,It provides no defensive framework; a model is just as vulnerable to malicious input.</p><p>Least-to-most,Breaking a complex problem down into smaller sub-problems.,&quot;This is a reasoning strategy. An attacker could still inject a &quot;&quot;sub-problem&quot;&quot; that hijacks the model.&quot;</p><p>Chain-of-thought,Encouraging the model to show its step-by-step reasoning.,&quot;While it improves logic, it can actually make injections easier to see, but it doesn&#39;t stop the model from following a malicious instruction.&quot;</p>",10)])])}const p=e(r,[["render",n]]),c=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/092-Adversarial-prompting.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/092-Adversarial-prompting.md"}');export{p as comp,c as data};
