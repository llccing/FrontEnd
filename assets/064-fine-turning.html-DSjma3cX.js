import{_ as i,c as a,b as e,o as n}from"./app-DxUSmTbc.js";const o={};function s(r,t){return n(),a("div",null,[...t[0]||(t[0]=[e("p",null,"A research company implemented a chatbot by using a foundation model （FM） from Amazon Bedrock. The chatbot searches for answers to questions from a large database of research papers. After multiple prompt engineering attempts, the company notices that the FM is performing poorly because of the complex scientific terms in the research papers. How can the company improve the performance of the chatbot? A.Use few-shot prompting to define how the FM can answer the questions. B.Use domain adaptation fine-tuning to adapt the FM to complex scientific terms. C.Change the FM inference parameters. D.Clean the research paper data to remove complex scientific terms.",-1),e("p",null,"The correct answer is B. Use domain adaptation fine-tuning to adapt the FM to complex scientific terms.",-1),e("p",null,"Here is the detailed explanation for each option and why B is the best choice:",-1),e("p",null,'Reason for the Correct Answer (B) Domain Adaptation Fine-tuning: This process involves taking a pre-trained Foundation Model (FM) and training it further on a specific, smaller dataset (in this case, the scientific research papers). Why it works: When a model struggles with "complex scientific terms," it means those terms were likely not well-represented in its original training data. Fine-tuning allows the model to learn the specific vocabulary, context, and relationships between terms in a specialized field. Amazon Bedrock supports fine-tuning for specific models to handle exactly this kind of "domain-specific" knowledge gap.',-1),e("p",null,"Analysis of the Other Options (Why they are incorrect)",-1),e("p",null,`A. Use few-shot prompting to define how the FM can answer the questions. Reasoning: Few-shot prompting involves giving the model a few examples of "Question and Answer" pairs within the prompt to show it how to behave. Why it fails here: While few-shot prompting is great for teaching the model a specific format or style, it is not effective for teaching a model a massive library of complex scientific terminology. If the model doesn’t "understand" the underlying language of the research papers, a few examples won't be enough to fix its lack of specialized knowledge.`,-1),e("p",null,"C. Change the FM inference parameters. Reasoning: Inference parameters include things like Temperature (randomness), Top P (diversity), and Max Tokens (length). Why it fails here: These parameters control the creativity and output style of the model, but they do not add new knowledge. Changing the temperature will not help the model understand a scientific term it doesn't recognize; it will simply make the model guess more or less creatively.",-1),e("p",null,`D. Clean the research paper data to remove complex scientific terms. Reasoning: This involves "dumbing down" the source data so the model can read it more easily. Why it fails here: This is counterproductive for a research company. If you remove the complex scientific terms, you are removing the actual substance and technical accuracy of the research. The chatbot's answers would become vague, oversimplified, or medically/scientifically incorrect.`,-1),e("p",null,"Summary for your Exam When you see a question where Prompt Engineering has already been tried and the problem is highly specialized vocabulary (medical, legal, scientific), the answer is almost always Fine-tuning (specifically Domain Adaptation).",-1)])])}const l=i(o,[["render",s]]),h=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/064-fine-turning.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212945000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"510851a2e40013ba249eb2696f9e38f188ca533a","time":1768212945000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Update README.md"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/064-fine-turning.md"}');export{l as comp,h as data};
