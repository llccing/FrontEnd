import{_ as t,c as o,a,o as n}from"./app-DxUSmTbc.js";const s={};function i(r,e){return n(),o("div",null,[...e[0]||(e[0]=[a('<h2 id="q21" tabindex="-1"><a class="header-anchor" href="#q21"><span>Q21</span></a></h2><p><strong>Answer:</strong> D</p><p>The solution that will meet these requirements with the least operational overhead is D: Use an Amazon S3 bucket to host the website&#39;s static content, deploy an Amazon CloudFront distribution, set the S3 bucket as the origin, and use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.</p><p>Using Amazon S3 to host static content and Amazon CloudFront to distribute the content can provide high performance and scale for websites with millions of requests each hour. Amazon API Gateway and AWS Lambda can be used to build scalable and highly available backend APIs to support the website, and Amazon DynamoDB can be used to store the data. This solution requires minimal operational overhead as it leverages fully managed services that automatically scale to meet demand.</p><hr><p>Option A is incorrect because using multiple S3 buckets to host the full website would not provide the required performance and scale for millions of requests each hour with millisecond latency.</p><p>Option B is incorrect because deploying the full website on EC2 instances and using an Application Load Balancer (ALB) and an RDS database would require more operational overhead to maintain and scale the infrastructure.</p><p>Option C is incorrect because while deploying the application in containers and hosting them on Amazon Elastic Kubernetes Service (EKS) can provide high performance and scale, it would require more operational overhead to maintain and scale the infrastructure compared to using fully managed services like S3 and CloudFront.</p><h2 id="q22" tabindex="-1"><a class="header-anchor" href="#q22"><span>Q22</span></a></h2><p><strong>Answer:</strong> B</p><p>The storage option that meets these requirements is B: S3 Intelligent-Tiering.</p><p>Amazon S3 Intelligent Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. It can store objects in two access tiers: the frequent access tier and the infrequent access tier. The frequent access tier is optimized for frequently accessed objects and is charged at the same rate as S3 Standard. The infrequent access tier is optimized for objects that are not accessed frequently and are charged at a lower rate than S3 Standard.</p><p>S3 Intelligent Tiering is a good choice for storing media files that are accessed frequently and infrequently in an unpredictable pattern because it automatically moves data to the most cost-effective storage tier based on access patterns, minimizing storage and retrieval costs. It is also resilient to the loss of an Availability Zone because it stores objects in multiple Availability Zones within a region.</p><p>Option A, S3 Standard, is not a good choice because it does not offer the cost optimization of S3 Intelligent-Tiering.</p><p>Option C, S3 Standard-Infrequent Access (S3 Standard-IA), is not a good choice because it is optimized for infrequently accessed objects and does not offer the cost optimization of S3 Intelligent-Tiering.</p><p>Option D, S3 One Zone-Infrequent Access (S3 One Zone-IA), is not a good choice because it is not resilient to the loss of an Availability Zone. It stores objects in a single Availability Zone, making it less durable than other storage classes.</p><h2 id="q23" tabindex="-1"><a class="header-anchor" href="#q23"><span>Q23</span></a></h2><p><strong>Answer:</strong> B</p><p>The storage solution that will meet these requirements most cost-effectively is B: Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.</p><p>Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 storage class for long-term retention of data that is rarely accessed and for which retrieval times of several hours are acceptable. It is the lowest-cost storage option in Amazon S3, making it a cost-effective choice for storing backup files that are not accessed after 1 month.</p><p>You can use an S3 Lifecycle configuration to automatically transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month. This will minimize the storage costs for the backup files that are not accessed frequently.</p><p>Option A, configuring S3 Intelligent-Tiering to automatically migrate objects, is not a good choice because it is not designed for long-term storage and does not offer the cost benefits of S3 Glacier Deep Archive.</p><p>Option C, transitioning objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.</p><p>Option D, transitioning objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.</p><h2 id="q24" tabindex="-1"><a class="header-anchor" href="#q24"><span>Q24</span></a></h2><p><strong>Answer:</strong> B</p><p>The solution that will meet these requirements with the least operational overhead is B: Use Cost Explorer&#39;s granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.</p><p>Cost Explorer&#39;s granular filtering feature allows you to filter and analyze EC2 costs based on instance types, regions, and other attributes. This feature provides detailed cost analysis and can help identify the root cause of vertical scaling.</p><p>Option A, using AWS Budgets to create a budget report and compare EC2 costs based on instance types, is not a good choice because it does not provide the granular filtering feature required for an in-depth analysis.</p><p>Option C, using graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months, is not a good choice because it does not provide the granular filtering feature required for an in-depth analysis.</p><p>Option D, using AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket, is not a good choice because it requires more operational overhead to set up and maintain.</p><p>https://aws.amazon.com/aws-cost-management/pricing/</p><h2 id="q25" tabindex="-1"><a class="header-anchor" href="#q25"><span>Q25</span></a></h2><p><strong>Answer:</strong> D</p><p>A - refactoring can be a solution, BUT requires a LOT of effort - not the answer B - DynamoDB is NoSQL and Aurora is SQL, so it requires a DB migration... again a LOT of effort, so no the answer C and D are similar in structure, but... C uses SNS, which would notify the 2nd Lambda function... provoking the same bottleneck... not the solution D uses SQS, so the 2nd lambda function can go to the queue when responsive to keep with the DB load process. Usually the app decoupling helps with the performance improvement by distributing load. In this case, the bottleneck is solved by uses queues... so D is the answer.</p><h2 id="q26" tabindex="-1"><a class="header-anchor" href="#q26"><span>Q26</span></a></h2><p><strong>Answer:</strong> A</p><p>The solution that will accomplish this goal is A: Turn on AWS Config with the appropriate rules.</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor and record changes to the configuration of your Amazon S3 buckets. By turning on AWS Config and enabling the appropriate rules, you can ensure that your S3 buckets do not have unauthorized configuration changes.</p><p>AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.</p><p>Amazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.</p><p>Amazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.</p><h2 id="q27" tabindex="-1"><a class="header-anchor" href="#q27"><span>Q27</span></a></h2><p><strong>Answer:</strong> A</p><p>Answere A : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html</p><p>Share a single dashboard and designate specific email addresses of the people who can view the dashboard. Each of these users creates their own password that they must enter to view the dashboard.</p><h2 id="q28" tabindex="-1"><a class="header-anchor" href="#q28"><span>Q28</span></a></h2><p><strong>Answer:</strong> B</p><p>Tricky question!!! forget one-way or two-way. In this scenario, AWS applications (Amazon Chime, Amazon Connect, Amazon QuickSight, AWS Single Sign-On, Amazon WorkDocs, Amazon WorkMail, Amazon WorkSpaces, AWS Client VPN, AWS Management Console, and AWS Transfer Family) need to be able to look up objects from the on-premises domain in order for them to function. This tells you that authentication needs to flow both ways. This scenario requires a two-way trust between the on-premises and AWS Managed Microsoft AD domains. It is a requirement of the application Scenario 2: https://aws.amazon.com/es/blogs/security/everything-you-wanted-to-know-about-trusts-with-aws-managed-microsoft-ad/</p><p>AWS IAM Identity Center requires a two-way trust so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to permission sets or applications. User and group metadata is also used by applications for collaboration, like when you share a dashboard with another user or group. The trust from AWS Directory Service for Microsoft Active Directory to your domain permits IAM Identity Center to trust your domain for authentication. The trust in the opposite direction grants AWS permissions to read user and group metadata.</p><h2 id="q29" tabindex="-1"><a class="header-anchor" href="#q29"><span>Q29</span></a></h2><p><strong>Answer:</strong> A</p><p>agree with A, Global Accelerator has automatic failover and is perfect for this scenario with VoIP https://aws.amazon.com/global-accelerator/faqs/</p><hr><p>CloudFront uses Edge Locations to cache content while Global Accelerator uses Edge Locations to find an optimal pathway to the nearest regional endpoint. CloudFront is designed to handle HTTP protocol meanwhile Global Accelerator is best used for both HTTP and non-HTTP protocols such as TCP and UDP. so i think A is a better answer</p><h2 id="q30" tabindex="-1"><a class="header-anchor" href="#q30"><span>Q30</span></a></h2><p><strong>Answer:</strong> C</p><p>C - Create a manual Snapshot of DB and shift to S3- Standard and Restore form Manual Snapshot when required.</p><p>Not A - By stopping the DB although you are not paying for DB hours you are still paying for Provisioned IOPs , the storage for Stopped DB is more than Snapshot of underlying EBS vol. and Automated Back ups . Not D - Is possible but not MOST cost effective, no need to run the RDS when not needed.</p><hr><p>Answer C, you still pay for storage when an RDS database is stopped</p>',61)])])}const h=t(s,[["render",i]]),d=JSON.parse('{"path":"/AWS/SAA/02Examtopics/Questions/0021-Answers.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212945000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"510851a2e40013ba249eb2696f9e38f188ca533a","time":1768212945000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Update README.md"}]},"filePathRelative":"AWS/SAA/02Examtopics/Questions/0021-Answers.md"}');export{h as comp,d as data};
