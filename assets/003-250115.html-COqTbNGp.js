import{_ as a,c as t,a as r,o as s}from"./app-DxUSmTbc.js";const n={};function i(o,e){return s(),t("div",null,[...e[0]||(e[0]=[r('<h1 id="result" tabindex="-1"><a class="header-anchor" href="#result"><span>Result</span></a></h1><p>Correct rate: 74% pass(more than 70%) time: 1h 26 mins correct questions: 48/65</p><h2 id="_1-17-approximatenumberofmessages-sqs-queue-attribute" tabindex="-1"><a class="header-anchor" href="#_1-17-approximatenumberofmessages-sqs-queue-attribute"><span>1/17 ApproximateNumberOfMessages SQS queue attribute.</span></a></h2><p>For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it&#39;s configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times. The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn&#39;t vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group.</p><p>Backlog per instance: To calculate your backlog per instance, start with the ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue (number of messages available for retrieval from the queue). Divide that number by the fleet&#39;s running capacity, which for an Auto Scaling group is the number of instances in the InService state, to get the backlog per instance.</p><h2 id="_2-17-auto-scaling-group-elb-rds-multi-az" tabindex="-1"><a class="header-anchor" href="#_2-17-auto-scaling-group-elb-rds-multi-az"><span>2/17 Auto Scaling Group &amp; ELB &amp; RDS multi-AZ</span></a></h2><h2 id="_3-17-disaster-recovery-plan" tabindex="-1"><a class="header-anchor" href="#_3-17-disaster-recovery-plan"><span>3/17 Disaster Recovery Plan</span></a></h2><p>Warm Standby, RTO in minutes, RPO in seconds.</p><h2 id="_4-17-aurora-serverless-is-high-performance-auto-scaling-and-cost-effective" tabindex="-1"><a class="header-anchor" href="#_4-17-aurora-serverless-is-high-performance-auto-scaling-and-cost-effective"><span>4/17 Aurora Serverless is high performance, auto-scaling, and cost-effective.</span></a></h2><h2 id="_5-17-global-accelerator" tabindex="-1"><a class="header-anchor" href="#_5-17-global-accelerator"><span>5/17 Global Accelerator</span></a></h2><p>exist: - Auto Scaling Group - EC2 instances - Application Load Balancer - Aurora</p><p>when periodic requests - Global Accelerator - CloudFront</p><h2 id="_6-17-low-latency-high-availability-in-another-region" tabindex="-1"><a class="header-anchor" href="#_6-17-low-latency-high-availability-in-another-region"><span>6/17 Low latency, high availability in another region.</span></a></h2><p>provision EC2 instances in another region and Application Load Balancer. and use Global Accelerator to route traffic to the instances.</p><h2 id="_7-17-eni-elastic-network-interface" tabindex="-1"><a class="header-anchor" href="#_7-17-eni-elastic-network-interface"><span>7/17 ENI, Elastic Network Interface</span></a></h2><p>the Network interface and the Elastic Network Interface are the same thing.</p><p>Low-budget, high-availability solution</p><p>If one of your instances serving a particular function fails, its network interface can be attached to a replacement or hot standby instance pre-configured for the same role in order to rapidly recover the service. For example, you can use a network interface as your primary or secondary network interface to a critical service such as a database instance or a NAT instance. If the instance fails, you (or more likely, the code running on your behalf) can attach the network interface to a hot standby instance. Because the interface maintains its private IP addresses, Elastic IP addresses, and MAC address, network traffic begins flowing to the standby instance as soon as you attach the network interface to the replacement instance. Users experience a brief loss of connectivity between the time the instance fails and the time that the network interface is attached to the standby instance, but no changes to the route table or your DNS server are required.</p><h2 id="_8-17-new-feature-aurora-serverless" tabindex="-1"><a class="header-anchor" href="#_8-17-new-feature-aurora-serverless"><span>8/17 new feature, Aurora Serverless</span></a></h2><p>for infrequent database access. and minimal downtime. and without selecting a particular instance type. just use Aurora Serverless for MySQL or PostgreSQL.</p><h2 id="_9-17-config-the-cloudfront-high-availability" tabindex="-1"><a class="header-anchor" href="#_9-17-config-the-cloudfront-high-availability"><span>9/17 config the cloudfront high availability</span></a></h2><p>use origin group.</p><h2 id="_10-17-vpc-endpoint-the-bucket-policy-to-restrict-access-to-the-bucket" tabindex="-1"><a class="header-anchor" href="#_10-17-vpc-endpoint-the-bucket-policy-to-restrict-access-to-the-bucket"><span>10/17 vpc endpoint the bucket policy to restrict access to the bucket.</span></a></h2><h2 id="_11-17-sse-kms-cmks" tabindex="-1"><a class="header-anchor" href="#_11-17-sse-kms-cmks"><span>11/17 sse-kms, cmks</span></a></h2><p>https://awstip.com/5-minutes-to-aws-s3-bucket-encryption-sse-c-sse-s3-sse-kms-e2fb07b05cb3</p><p>all three types of keys share a common ground — the ‘SSE’ world, which stands for ‘Server-Side Encryption’. This means that the server will handle encrypting and decrypting the data, rather than the customer (see ‘Client-Side Encryption’ instad). The key difference between them (Pun intended!) is how the key handled by AWS (SSE-KMS,SSE-S3)or you (SSE-C )</p><h2 id="_12-17-deregister-targets-from-target-groups" tabindex="-1"><a class="header-anchor" href="#_12-17-deregister-targets-from-target-groups"><span>12/17 deregister targets from target groups</span></a></h2><p>when you scale-in and worry about some service need to continue running. you can change the deregistration timeout for the target groiup of the instance to greater than 1500 seconds.(1500/60 = 25 mins)</p><p>If demand on your application decreases, or you need to service your targets, you can deregister targets from your target groups. Deregistering a target removes it from your target group, but does not affect the target otherwise. The load balancer stops routing requests to a target as soon as it is deregistered. The target enters the draining state until in-flight requests have completed. You can register the target with the target group again when you are ready for it to resume receiving requests.</p><h2 id="_13-17-the-cloudwatch-logs-can-send-to-kinesis-lambda-or-kinesis-data-firehose" tabindex="-1"><a class="header-anchor" href="#_13-17-the-cloudwatch-logs-can-send-to-kinesis-lambda-or-kinesis-data-firehose"><span>13/17 the cloudwatch logs can send to kinesis, lambda, or kinesis data firehose.</span></a></h2><h2 id="_14-17-s3-glacier-instant-retrieval-is-lowest-cost-storage-for-long-lived-data-that-is-rarely-accessed-and-requires-retrieval-in-milliseconds" tabindex="-1"><a class="header-anchor" href="#_14-17-s3-glacier-instant-retrieval-is-lowest-cost-storage-for-long-lived-data-that-is-rarely-accessed-and-requires-retrieval-in-milliseconds"><span>14/17 s3 glacier instant retrieval is lowest cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds.</span></a></h2><h2 id="_15-17-s3-event-notification" tabindex="-1"><a class="header-anchor" href="#_15-17-s3-event-notification"><span>15/17 S3 event notification</span></a></h2><p>S3 event notifications enable you to receive notifications when certain events happen in your S3 bucket. These notifications are delivered to an SNS topic, an SQS queue, or an AWS Lambda function.</p><p>https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html</p><h2 id="_16-17-remember-auto-scaling-group-elb-rds-multi-az" tabindex="-1"><a class="header-anchor" href="#_16-17-remember-auto-scaling-group-elb-rds-multi-az"><span>16/17 remember auto scaling group + ELB + RDS multi-AZ.</span></a></h2><h2 id="_17-17-only-nlb-support-udp-protocol" tabindex="-1"><a class="header-anchor" href="#_17-17-only-nlb-support-udp-protocol"><span>17/17 only NLB support UDP protocol.</span></a></h2><p>so you should choose NLB and Global Accelerator for UDP.</p>',37)])])}const h=a(n,[["render",i]]),l=JSON.parse('{"path":"/AWS/SAA/03PreExams/003-250115.html","title":"Result","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212945000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"510851a2e40013ba249eb2696f9e38f188ca533a","time":1768212945000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Update README.md"}]},"filePathRelative":"AWS/SAA/03PreExams/003-250115.md"}');export{h as comp,l as data};
