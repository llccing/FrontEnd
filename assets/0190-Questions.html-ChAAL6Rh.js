import{_ as a,c as t,a as n,o}from"./app-Dbw06csz.js";const i={};function s(r,e){return o(),t("div",null,[...e[0]||(e[0]=[n('<h2 id="q191" tabindex="-1"><a class="header-anchor" href="#q191"><span>Q191</span></a></h2><p>A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to eliminate the timeouts without preventing employees from performing queries.</p><p>What should a solutions architect do to meet these requirements?</p><p>A. Create a read replica. Move reporting queries to the read replica. B. Create a read replica. Distribute the ordering application to the primary DB instance and the read replica. C. Migrate the ordering application to Amazon DynamoDB with on-demand capacity. D. Schedule the reporting queries for non-peak hours.</p><h2 id="q192" tabindex="-1"><a class="header-anchor" href="#q192"><span>Q192</span></a></h2><p>A hospital wants to create digital copies for its large collection of historical written records. The hospital will continue to add hundreds of new documents each day. The hospital’s data team will scan the documents and will upload the documents to the AWS Cloud.</p><p>A solutions architect must implement a solution to analyze the documents, extract the medical information, and store the documents so that an application can run SQL queries on the data. The solution must maximize scalability and operational efficiency.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p><p>A. Write the document information to an Amazon EC2 instance that runs a MySQL database. B. Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data. C. Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the medical information. D. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract relevant medical information from the text. E. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant medical information from the text.</p><h2 id="q193" tabindex="-1"><a class="header-anchor" href="#q193"><span>Q193</span></a></h2><p>A company is running a batch application on Amazon EC2 instances. The application consists of a backend with multiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions architect must reduce the number of database reads while ensuring high availability.</p><p>What should the solutions architect do to meet this requirement?</p><p>A. Add Amazon RDS read replicas. B. Use Amazon ElastiCache for Redis. C. Use Amazon Route 53 DNS caching D. Use Amazon ElastiCache for Memcached.</p><h2 id="q194" tabindex="-1"><a class="header-anchor" href="#q194"><span>Q194</span></a></h2><p>A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs.</p><p>Which solution will meet these requirements?</p><p>A. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication. B. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs. C. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region. D. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.</p><h2 id="q195" tabindex="-1"><a class="header-anchor" href="#q195"><span>Q195</span></a></h2><p>A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.</p><p>What should a solutions architect do to meet these requirements?</p><p>A. Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task. B. Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint. C. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue. D. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command.</p><h2 id="q196" tabindex="-1"><a class="header-anchor" href="#q196"><span>Q196</span></a></h2><p>A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort.</p><p>Which solution meets these requirements?</p><p>A. Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days, and delete the original stack. B. Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days. C. Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days. D. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.</p><h2 id="q197" tabindex="-1"><a class="header-anchor" href="#q197"><span>Q197</span></a></h2><p>A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available.</p><p>Which combination of actions should the company take to meet these requirements? (Choose two.)</p><p>A. Refactor the application as serverless with AWS Lambda functions running .NET Core. B. Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment. C. Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI). D. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon DynamoDB in a Multi-AZ deployment. E. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.</p><h2 id="q198" tabindex="-1"><a class="header-anchor" href="#q198"><span>Q198</span></a></h2><p>A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead.</p><p>Which solution meets these requirements?</p><p>A. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage. B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage C. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage. D. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.</p><h2 id="q199" tabindex="-1"><a class="header-anchor" href="#q199"><span>Q199</span></a></h2><p>A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes.</p><p>Which solution will meet these requirements?</p><p>A. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis. B. Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis. C. Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis. D. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.</p><h2 id="q200" tabindex="-1"><a class="header-anchor" href="#q200"><span>Q200</span></a></h2><p>A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p><p>A. Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request. B. For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function. C. Send the user’s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access. D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.</p>',41)])])}const h=a(i,[["render",s]]),m=JSON.parse('{"path":"/AWS/SAA/02Examtopics/Questions/0190-Questions.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/SAA/02Examtopics/Questions/0190-Questions.md"}');export{h as comp,m as data};
