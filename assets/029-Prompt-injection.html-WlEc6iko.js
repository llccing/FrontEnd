import{_ as t,c as o,a as n,o as a}from"./app-Dbw06csz.js";const r={};function i(s,e){return a(),o("div",null,[...e[0]||(e[0]=[n("<p>29.A company wants to use a large language model （LLM） to develop a conversational agent. The company needs to prevent the LLM from being manipulated with common prompt engineering techniques to perform undesirable actions or expose sensitive information. Which action will reduce these risks? A.Create a prompt template that teaches the LLM to detect attack patterns. B.Increase the temperature parameter on invocation requests to the LLM. C.Avoid using LLMs that are not listed in Amazon SageMaker. D.Decrease the number of input tokens on invocations of the LLM.</p><p><strong>A. Create a prompt template that teaches the LLM to detect attack patterns.</strong></p><p>Using a strong system/prompt template (guardrails) that explicitly instructs the model to <strong>recognize and refuse prompt-injection/jailbreak patterns</strong> and to <strong>never reveal sensitive data</strong> is the most direct way among these options to reduce manipulation risk.</p><ul><li><strong>B</strong> (increase temperature) generally makes outputs <em>less predictable</em> and can increase risk.</li><li><strong>C</strong> is unrelated to prompt-injection resilience.</li><li><strong>D</strong> (fewer input tokens) might limit context but doesn’t meaningfully stop common prompt attacks.</li></ul>",4)])])}const m=t(r,[["render",i]]),l=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/029-Prompt-injection.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/029-Prompt-injection.md"}');export{m as comp,l as data};
