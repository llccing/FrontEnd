import{_ as t,c as o,a as r,o as a}from"./app-Dbw06csz.js";const s={};function i(n,e){return a(),o("div",null,[...e[0]||(e[0]=[r("<p>A company has fine-tuned a large language model （LLM） to answer questions for a help desk. The company wants to determine if the fine-tuning has enhanced the model&#39;s accuracy. Which metric should the company use for the evaluation? A.Precision B.Time to first token C.F1 score D.Word error rate</p><p>The correct answer is C. F1 score.</p><p>In the context of evaluating large language models (LLMs) for tasks like a help desk (Question Answering), the F1 score is a standard metric used to measure accuracy. It provides a balanced assessment of how well the model&#39;s response matches the ground-truth answer.</p><p>Why F1 Score is the Best Choice For text-based tasks, &quot;accuracy&quot; isn&#39;t a simple &quot;yes/no&quot; calculation. The F1 score is used because it combines two critical factors:</p><p>Precision: How much of the model&#39;s generated answer is relevant and correct?</p><p>Recall: Did the model capture all the important information from the reference answer?</p><p>By taking the harmonic mean of these two, the F1 score rewards models that are both concise (high precision) and comprehensive (high recall), which is ideal for a help desk environment where users need both correct and complete information.</p><p>Why the other options are incorrect:</p><p>Metric,Primary Use Case,Why it&#39;s not the best for LLM accuracy</p><p>Precision,Classification / Retrieval,&quot;Precision alone ignores &quot;&quot;Recall.&quot;&quot; A model could give a very short, one-word correct answer and have 100% precision, but fail to actually help the user (low recall).&quot;</p><p>Time to first token,Performance / Latency,&quot;This measures speed, not correctness. It tells you how fast the model starts responding, which is irrelevant to whether the answer is accurate.&quot;</p><p>Word error rate,Speech-to-Text (ASR),&quot;This measures the number of insertions, deletions, or substitutions. It is used to see how well a transcript matches audio, not how well an AI understands and answers a question.&quot;</p><p>Common Evaluation Pair</p><p>In professional machine learning evaluations (such as the SQuAD benchmark), the F1 score is almost always paired with Exact Match (EM). While EM requires the model&#39;s answer to be character-perfect, the F1 score is preferred in help desk scenarios because it is more &quot;forgiving&quot;—it recognizes when a model provides the right information even if the wording or phrasing differs slightly from the reference.</p>",14)])])}const h=t(s,[["render",i]]),l=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/093-judge-fine-tuned-LLM.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/093-judge-fine-tuned-LLM.md"}');export{h as comp,l as data};
