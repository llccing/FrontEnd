import{_ as e,c as i,a as n,o as a}from"./app-Dbw06csz.js";const o={};function r(s,t){return a(),i("div",null,[...t[0]||(t[0]=[n('<p>A company is building a customer service chatbot. The company wants the chatbot to improve its responses by learning from past interactions and online resources. Which Al learning strategy provides this self-improvement capability? A.Supervised learning with a manually curated dataset of good responses and bad responses B.Reinforcement learning with rewards for positive customer feedback C.Unsupervised learning to find clusters of similar customer inquiries D.Supervised learning with a continuously updated FAQ database</p><p>The correct answer is B. Reinforcement learning with rewards for positive customer feedback.</p><p>Here is a detailed explanation of why this is the correct choice and why the other options do not fit the requirement of &quot;self-improvement&quot; through interaction.</p><p>Correct Answer Breakdown</p><p>B. Reinforcement learning with rewards for positive customer feedback Reasoning: Reinforcement Learning (RL) is based on an agent (the chatbot) taking an action (providing a response) in an environment (the customer chat). It then receives a reward or penalty based on the outcome (the feedback). Why it fits: The question asks for a strategy where the bot &quot;improves by learning from past interactions.&quot; In RL, if a customer gives a &quot;thumbs up&quot; or says &quot;thank you,&quot; the AI receives a positive reward signal. Over time, the AI learns to prioritize the patterns of behavior that lead to those rewards, effectively &quot;self-improving&quot; without a human having to manually label every new data point.</p><p>Explanation of Other Options (The &quot;Distractors&quot;)</p><p>A. Supervised learning with a manually curated dataset... Reasoning: Supervised learning requires a human to provide a specific input and the &quot;correct&quot; output (labels). Why it’s wrong: While this is how most bots are initially trained, it does not provide &quot;self-improvement&quot; from interactions. If the bot encounters a new scenario, it cannot learn on its own; a human must first manually curate and label the new data before the bot can be retrained. It is a static, manual process.</p><p>C. Unsupervised learning to find clusters of similar customer inquiries Reasoning: Unsupervised learning is used to find hidden patterns or groupings (clusters) in data without any labels or feedback. Why it’s wrong: Clustering can help a company understand what customers are talking about (e.g., &quot;Group A is asking about refunds, Group B is asking about shipping&quot;), but it doesn&#39;t teach the chatbot how to improve its response. It just organizes the data; it doesn&#39;t provide a mechanism for the bot to get &quot;better&quot; at its job.</p><p>D. Supervised learning with a continuously updated FAQ database Reasoning: This is a variation of Option A. It involves feeding the bot known answers to specific questions. Why it’s wrong: This is essentially &quot;Retrieval-Augmented Generation&quot; (RAG) or simple keyword matching. While the information the bot provides might stay current (updated FAQ), the learning strategy itself isn&#39;t improving through interaction. The bot is just looking up information in a newer book; it isn&#39;t learning how to be a better communicator based on customer success.</p><p>Summary Table for the Exam</p><table><thead><tr><th style="text-align:left;">Strategy</th><th style="text-align:left;">Primary Mechanism</th><th style="text-align:left;">Best Used For...</th></tr></thead><tbody><tr><td style="text-align:left;">Supervised</td><td style="text-align:left;">Learning from labeled &quot;right/wrong&quot; pairs.</td><td style="text-align:left;">Initial training on known data.</td></tr><tr><td style="text-align:left;">Unsupervised</td><td style="text-align:left;">Finding patterns in unlabeled data.</td><td style="text-align:left;">Identifying trends or customer segments.</td></tr><tr><td style="text-align:left;">Reinforcement</td><td style="text-align:left;">Trial and error based on rewards.</td><td style="text-align:left;">Continuous self-improvement in dynamic environments.</td></tr></tbody></table>',11)])])}const d=e(o,[["render",r]]),u=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/058-RL.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/058-RL.md"}');export{d as comp,u as data};
