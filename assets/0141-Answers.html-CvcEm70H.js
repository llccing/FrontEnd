import{_ as a,c as t,a as o,o as s}from"./app-Dbw06csz.js";const n={};function i(r,e){return s(),t("div",null,[...e[0]||(e[0]=[o('<h2 id="q141" tabindex="-1"><a class="header-anchor" href="#q141"><span>Q141</span></a></h2><p><strong>Answer:</strong> A Answer is A. Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content https://www.examtopics.com/discussions/amazon/view/81081-exam-aws-certified-solutions-architect-associate-saa-c02/</p><hr><p>Amazon cloud front is a better choice in terms of delivering both static and dynamic content globally. Also option B says deploy the application stack to only two regions. but the use case is to access the portal globally. Don&#39;t think Amazon Route 53 latency routing policy will have a bigger impact in terms of low latency.</p><h2 id="q142" tabindex="-1"><a class="header-anchor" href="#q142"><span>Q142</span></a></h2><p><strong>Answer:</strong> C</p><p>Correct Answer: C AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p><hr><p>UDP, static IP = Global Accelerator and Network Load Balancer</p><h2 id="q143" tabindex="-1"><a class="header-anchor" href="#q143"><span>Q143</span></a></h2><p><strong>Answer:</strong> D</p><p>D is organic pattern, lift and shift, decompose to containers, first making most use of existing code, whilst new features can be added over time with lambda+api gw later. A is leapfrog pattern. requiring refactoring all code up front.o</p><h2 id="q144" tabindex="-1"><a class="header-anchor" href="#q144"><span>Q144</span></a></h2><p><strong>Answer:</strong> B</p><p>Report = Aurora replica</p><h2 id="q145" tabindex="-1"><a class="header-anchor" href="#q145"><span>Q145</span></a></h2><p><strong>Answer:</strong> D</p><p>I was tempted to pick A but then I realized there are two key requirements:</p><ul><li>scale seamlessly</li><li>cost-effectively</li></ul><p>None of A-C give seamless scalability. A and B are about adding second instance (which I assume does not match to &quot;scale seamlessly&quot;). C is about changing instance type.</p><p>Therefore D is only workable solution to the scalability requirement</p><h2 id="q146" tabindex="-1"><a class="header-anchor" href="#q146"><span>Q146</span></a></h2><p><strong>Answer:</strong> B</p><p>Answer is B: Reserved is cheaper than on demand the company has. And it&#39;s meet the availabilty (HA) requirement as to spot instance that can be disrupted at any time. PRICING BELOW. On-Demand: 0% There’s no commitment from you. You pay the most with this option. Reserved : 40%-60%1-year or 3-year commitment from you. You save money from that commitment. Spot 50%-90% Ridiculously inexpensive because there’s no commitment from the AWS side.</p><h2 id="q147" tabindex="-1"><a class="header-anchor" href="#q147"><span>Q147</span></a></h2><p><strong>Answer:</strong> B</p><p>Ans B - S3 with Glacier Deep plus Lifecycle management</p><hr><p>B is the most cost-effective solution. Storing the logs in S3 and using S3 Lifecycle policies to transition logs older than 1 month to S3 Glacier Deep Archive allows for cost optimization based on data access patterns. Since logs older than 1 month are rarely accessed, moving them to S3 Glacier Deep Archive helps minimize storage costs while still retaining the logs for the required 10-year period.</p><p>A is incorrect because using AWS Backup to move logs to S3 Glacier Deep Archive can incur additional costs and complexity compared to using S3 Lifecycle policies directly.</p><p>C adds unnecessary complexity and costs by involving CloudWatch Logs and AWS Backup when direct management through S3 is sufficient.</p><p>D is incorrect because using S3 Lifecycle policies to move logs from CloudWatch Logs to S3 Glacier Deep Archive is not a valid option. CloudWatch Logs and S3 have separate storage mechanisms, and S3 Lifecycle policies cannot be applied directly to CloudWatch Logs.</p><h2 id="q148" tabindex="-1"><a class="header-anchor" href="#q148"><span>Q148</span></a></h2><p><strong>Answer:</strong> D</p><p><em>ensure that all notifications are eventually processed</em></p><hr><p>This is why https://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html</p><hr><p>I choose Option D as the correct answer.</p><p>To ensure that all notifications are eventually processed, the solutions architect can set up an Amazon SQS queue as the on-failure destination for the Amazon SNS topic. This way, when the Lambda function fails due to network connectivity issues, the notification will be sent to the queue instead of being lost. The Lambda function can then be modified to process messages in the queue, ensuring that all notifications are eventually processed.</p><h2 id="q149" tabindex="-1"><a class="header-anchor" href="#q149"><span>Q149</span></a></h2><p><strong>Answer:</strong> A Ans A - SQS (FIFO) ensures data is processed in the order it is received</p><h2 id="q150" tabindex="-1"><a class="header-anchor" href="#q150"><span>Q150</span></a></h2><p><strong>Answer:</strong> A</p><p>Composite alarms determine their states by monitoring the states of other alarms. You can <strong>use composite alarms to reduce alarm noise</strong>. For example, you can create a composite alarm where the underlying metric alarms go into ALARM when they meet specific conditions. You then can set up your composite alarm to go into ALARM and send you notifications when the underlying metric alarms go into ALARM by configuring the underlying metric alarms never to take actions. Currently, composite alarms can take the following actions: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html</p><hr><p>By creating composite alarms in CloudWatch, the solutions architect can combine multiple metrics, such as CPU utilization and read IOPS, into a single alarm. This allows the company to take action only when both conditions are met, reducing false alarms and focusing on meaningful alerts.</p><p>B can help in monitoring the overall health and performance of the application. However, it does not directly address the specific requirement of triggering an action when CPU utilization and read IOPS exceed certain thresholds simultaneously.</p><p>C. Creating CloudWatch Synthetics canaries is useful for actively monitoring the application&#39;s behavior and availability. However, it does not directly address the specific requirement of monitoring CPU utilization and read IOPS to trigger an action.</p><p>D. Creating single CloudWatch metric alarms with multiple metric thresholds where possible can be an option, but it does not address the requirement of triggering an action only when both CPU utilization and read IOPS exceed their respective thresholds simultaneously.</p>',50)])])}const l=a(n,[["render",i]]),h=JSON.parse('{"path":"/AWS/SAA/02Examtopics/Questions/0141-Answers.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/SAA/02Examtopics/Questions/0141-Answers.md"}');export{l as comp,h as data};
