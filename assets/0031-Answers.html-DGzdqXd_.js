import{_ as a,c as n,a as t,o}from"./app-Dbw06csz.js";const i={};function s(r,e){return o(),n("div",null,[...e[0]||(e[0]=[t('<h2 id="q31" tabindex="-1"><a class="header-anchor" href="#q31"><span>Q31</span></a></h2><p><strong>Answer:</strong> A</p><p>AWS Config provides a set of pre-built or customizable rules that can be used to check the configuration and compliance of AWS resources. By creating a custom rule or using the built-in rule for tagging, you can define the required tags for EC2, RDS DB and Redshift clusters. AWS Config continuously monitors the resources and generates configuration change events or evaluation results.</p><p>By leveraging AWS Config, the solution can automatically detect any resources that do not comply with the defined tagging requirements. This approach eliminates the need for manual checks or periodic code execution, reducing operational overhead. Additionally, AWS Config provides the ability to automatically remediate non-compliant resources by triggering Lambda or sending notifications, further streamlining the configuration management process.</p><p>Option B (using Cost Explorer) primarily focuses on cost analysis and does not provide direct enforcement of proper tagging. Option C and D (writing API calls and running them manually or through scheduled Lambda) require more manual effort and maintenance compared to using AWS Config rules.</p><h2 id="q32" tabindex="-1"><a class="header-anchor" href="#q32"><span>Q32</span></a></h2><p><strong>Answer:</strong> B</p><p>Good answer is B: client-side JavaScript. the website is static, so it must be S3.</p><h2 id="q33" tabindex="-1"><a class="header-anchor" href="#q33"><span>Q33</span></a></h2><p><strong>Answer:</strong> C</p><p>I would go for C. The tricky phrase is &quot;near-real-time solution&quot;, pointing to Firehouse, but it can&#39;t send data to DynamoDB, so it leaves us with C as best option.</p><hr><p>Q: What is a destination in Firehose?</p><p>A destination is the data store where your data will be delivered. Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumo Logic, LogicMonitor, MongoDB, and HTTP End Point as destinations.</p><hr><p>A: DynamoDB streams are logs, not fit for real-time sharing. B: S3 is not document database, it&#39;s BLOB D: S3 and files are not database C: Kinesis + Lambda + DynamoDB is high performance, low latency scalable solution.</p><h2 id="q34" tabindex="-1"><a class="header-anchor" href="#q34"><span>Q34</span></a></h2><p><strong>Answer:</strong> B</p><p>CloudTrail - Track user activity and API call history. Config - Assess, audits, and evaluates the configuration and relationships of tag resources.</p><p>Therefore, the answer is B</p><h2 id="q35" tabindex="-1"><a class="header-anchor" href="#q35"><span>Q35</span></a></h2><p><strong>Answer:</strong> D</p><p>A: GuardDuty is not for this, mostly for account monitoring for suspicious activity B: Inspector is for OS vulnerabilities C: Shield with R53 is not going to protect against DDoS D: Shield Advanced is build for DDoS protection</p><hr><p>Prevent large scale DDOS attack = AWS Shield Advanced</p><h2 id="q36" tabindex="-1"><a class="header-anchor" href="#q36"><span>Q36</span></a></h2><p><strong>Answer:</strong> B</p><p>it is all about multi-region key. AWS KMS supports multi-Region keys, which are AWS KMS keys in different AWS Regions that can be used interchangeably â€“ as though you had the same key in multiple Regions. Each set of related multi-Region keys has the same key material and key ID, so you can encrypt data in one AWS Region and decrypt it in a different AWS Region without re-encrypting or making a cross-Region call to AWS KMS. You can use multi-Region keys with client-side encryption libraries such as S3 client-side encryption https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</p><h2 id="q37" tabindex="-1"><a class="header-anchor" href="#q37"><span>Q37</span></a></h2><p><strong>Answer:</strong> B</p><p>How can Session Manager benefit my organization? Ans: No open inbound ports and no need to manage bastion hosts or SSH keys https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</p><hr><p>Option A provides direct access to the terminal interface of each instance, but it may not be practical for administration purposes and can be cumbersome to manage, especially for multiple instances.</p><p>Option C adds operational overhead and introduces additional infrastructure that needs to be managed, monitored, and secured. It also requires SSH key management and maintenance.</p><p>Option D is complex and may not be necessary for remote administration. It also requires administrators to connect from their local on-premises machines, which adds complexity and potential security risks.</p><p>Therefore, option B is the recommended solution as it provides secure, auditable, and repeatable remote access using IAM roles and AWS Systems Manager Session Manager, with minimal operational overhead.</p><h2 id="q38" tabindex="-1"><a class="header-anchor" href="#q38"><span>Q38</span></a></h2><p><strong>Answer:</strong> C</p><p>Option A (replicating the S3 bucket to all AWS Regions) can be costly and complex, requiring replication of data across multiple Regions and managing synchronization. It may not provide a significant latency improvement compared to the CloudFront solution.</p><p>Option B (provisioning accelerators in AWS Global Accelerator) can be more expensive as it adds an extra layer of infrastructure (accelerators) and requires associating IP addresses with the S3 bucket. CloudFront already includes global edge locations and provides similar acceleration capabilities.</p><p>Option D (enabling S3 Transfer Acceleration) can help improve upload speed to the S3 bucket but may not have a significant impact on reducing latency for website visitors.</p><p>Therefore, option C is the most cost-effective solution as it leverages CloudFront&#39;s caching and global distribution capabilities to decrease latency and improve website performance.</p><h2 id="q39" tabindex="-1"><a class="header-anchor" href="#q39"><span>Q39</span></a></h2><p><strong>Answer:</strong> A</p><p>A: Made for high levels of I/O opps for consistent, predictable performance. B: Can improve performance of insert opps, but it&#39;s a storage performance rather than processing power problem C: for moderate CPU usage D: for scale read-only replicas and doesn&#39;t improve performance of insert opps on the primary DB instance</p><h2 id="q40" tabindex="-1"><a class="header-anchor" href="#q40"><span>Q40</span></a></h2><p><strong>Answer:</strong> A</p><p>Definitely A, it&#39;s the most operationally efficient compared to D, which requires a lot of code and infrastructure to maintain. A is mostly managed (firehose is fully managed and S3 lifecycles are also managed)</p>',48)])])}const p=a(i,[["render",s]]),l=JSON.parse('{"path":"/AWS/SAA/02Examtopics/Questions/0031-Answers.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/SAA/02Examtopics/Questions/0031-Answers.md"}');export{p as comp,l as data};
