import{_ as t,c as a,a as o,o as i}from"./app-Dbw06csz.js";const n={};function r(s,e){return i(),a("div",null,[...e[0]||(e[0]=[o("<p>A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images. Which solution will meet these requirements? A.Implement moderation APIs. B.Retrain the model with a general public dataset. C.Perform model validation. D.Automate user feedback integration.</p><p>The correct answer is A. Implement moderation APIs.</p><p>Here is the detailed explanation for why this is the correct choice and why the other options are less suitable for this specific requirement.</p><p>Correct Answer: A. Implement moderation APIs</p><p>Reason: Moderation APIs (such as Amazon Rekognition for images) are specifically designed to detect inappropriate, suggestive, or offensive content. When a chatbot generates or retrieves an image, it can be passed through the moderation API before being shown to the user. If the API flags the content as &quot;unwanted,&quot; the chatbot can block the image and provide a safe alternative or an error message. Why it fits: This is a real-time, proactive guardrail that ensures the output adheres to safety policies regardless of how the model was trained.</p><p>Explanation of Incorrect Options</p><p>B. Retrain the model with a general public dataset. Reason for rejection: General public datasets (like those scraped from the internet) often contain high amounts of biased, inappropriate, or low-quality content. Retraining a model on a public dataset would likely make the problem worse, not better. Even if the dataset is curated, retraining is a very expensive and time-consuming process that does not provide a 100% guarantee that the model won&#39;t generate a &quot;bad&quot; image through a new combination of prompts.</p><p>C. Perform model validation. Reason for rejection: Model validation is a step in the development process used to check the model&#39;s accuracy and performance against a test dataset. While validation helps you understand if the model is working correctly, it is a diagnostic tool, not a preventative tool. It doesn&#39;t act as a filter for live user interactions.</p><p>D. Automate user feedback integration. Reason for rejection: User feedback (like &quot;reporting&quot; an image) is reactive. For a user to give feedback on an inappropriate image, they must first see the inappropriate image. The requirement is to ensure the chatbot does not return these images in the first place. Feedback is useful for long-term improvement (RLHF), but it does not solve the immediate need for a safety barrier.</p><p>AWS Context (For the Exam) In the context of AWS, this question is testing your knowledge of Amazon Rekognition Content Moderation. AWS recommends using a &quot;layered&quot; approach to AI safety, where the model generates the output, but a specialized service (the Moderation API) acts as a gatekeeper to protect the end-user.</p>",10)])])}const l=t(n,[["render",r]]),d=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/060-find-offensive.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/060-find-offensive.md"}');export{l as comp,d as data};
