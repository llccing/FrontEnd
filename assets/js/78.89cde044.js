(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{361:function(t,e,n){"use strict";n.r(e);var i=n(14),a=Object(i.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("A company is building a customer service chatbot. The company wants the  chatbot to improve its responses by learning from past interactions and online  resources. Which Al learning strategy provides this self-improvement capability?\nA.Supervised learning with a manually curated dataset of good responses and bad  responses\nB.Reinforcement learning with rewards for positive customer feedback C.Unsupervised learning to find clusters of similar customer inquiries D.Supervised learning with a continuously updated FAQ database")]),t._v(" "),e("p",[t._v("The correct answer is B. Reinforcement learning with rewards for positive customer feedback.")]),t._v(" "),e("p",[t._v('Here is a detailed explanation of why this is the correct choice and why the other options do not fit the requirement of "self-improvement" through interaction.')]),t._v(" "),e("p",[t._v("Correct Answer Breakdown")]),t._v(" "),e("p",[t._v('B. Reinforcement learning with rewards for positive customer feedback\nReasoning: Reinforcement Learning (RL) is based on an agent (the chatbot) taking an action (providing a response) in an environment (the customer chat). It then receives a reward or penalty based on the outcome (the feedback).\nWhy it fits: The question asks for a strategy where the bot "improves by learning from past interactions." In RL, if a customer gives a "thumbs up" or says "thank you," the AI receives a positive reward signal. Over time, the AI learns to prioritize the patterns of behavior that lead to those rewards, effectively "self-improving" without a human having to manually label every new data point.')]),t._v(" "),e("p",[t._v('Explanation of Other Options (The "Distractors")')]),t._v(" "),e("p",[t._v('A. Supervised learning with a manually curated dataset...\nReasoning: Supervised learning requires a human to provide a specific input and the "correct" output (labels).\nWhy it’s wrong: While this is how most bots are initially trained, it does not provide "self-improvement" from interactions. If the bot encounters a new scenario, it cannot learn on its own; a human must first manually curate and label the new data before the bot can be retrained. It is a static, manual process.')]),t._v(" "),e("p",[t._v('C. Unsupervised learning to find clusters of similar customer inquiries\nReasoning: Unsupervised learning is used to find hidden patterns or groupings (clusters) in data without any labels or feedback.\nWhy it’s wrong: Clustering can help a company understand what customers are talking about (e.g., "Group A is asking about refunds, Group B is asking about shipping"), but it doesn\'t teach the chatbot how to improve its response. It just organizes the data; it doesn\'t provide a mechanism for the bot to get "better" at its job.')]),t._v(" "),e("p",[t._v("D. Supervised learning with a continuously updated FAQ database\nReasoning: This is a variation of Option A. It involves feeding the bot known answers to specific questions.\nWhy it’s wrong: This is essentially \"Retrieval-Augmented Generation\" (RAG) or simple keyword matching. While the information the bot provides might stay current (updated FAQ), the learning strategy itself isn't improving through interaction. The bot is just looking up information in a newer book; it isn't learning how to be a better communicator based on customer success.")]),t._v(" "),e("p",[t._v("Summary Table for the Exam")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Strategy")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Primary Mechanism")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Best Used For...")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("Supervised")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v('Learning from labeled "right/wrong" pairs.')]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Initial training on known data.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("Unsupervised")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Finding patterns in unlabeled data.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Identifying trends or customer segments.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("Reinforcement")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Trial and error based on rewards.")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Continuous self-improvement in dynamic environments.")])])])])])}),[],!1,null,null,null);e.default=a.exports}}]);