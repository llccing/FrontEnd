(window.webpackJsonp=window.webpackJsonp||[]).push([[100],{399:function(t,e,i){"use strict";i.r(e);var o=i(14),n=Object(o.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("Which prompting technique can protect against prompt injection attacks?\nA.Adversarial prompting\nB.Zero-shot prompting\nC.Least-to-most prompting\nD.Chain-of-thought prompting")]),t._v(" "),e("p",[t._v("The correct answer is A. Adversarial prompting.")]),t._v(" "),e("p",[t._v("While adversarial prompting is often used as a method to test or execute attacks, it is also a primary technique used by developers to protect against prompt injection. By using adversarial techniques during the training or fine-tuning phase, developers can teach a model to recognize and ignore malicious instructions.")]),t._v(" "),e("p",[t._v("Why Adversarial Prompting?")]),t._v(" "),e("p",[t._v("In the context of defense, this is often referred to as Adversarial Robustness Training. It involves:")]),t._v(" "),e("ul",[e("li",[t._v('Red Teaming: Proactively trying to "break" the model with injection attacks to identify vulnerabilities.')]),t._v(" "),e("li",[t._v("Instructional Guardrails: Teaching the model to prioritize its system instructions over user-provided data that attempts to override them.")]),t._v(" "),e("li",[t._v("Input Sanitization: Crafting prompts that explicitly tell the model to treat user input as data only, not as commands.")])]),t._v(" "),e("p",[t._v("Why the other options are incorrect:")]),t._v(" "),e("p",[t._v("Technique,Description,Why it doesn't protect against injection\nZero-shot,Asking a model to perform a task without any examples.,It provides no defensive framework; a model is just as vulnerable to malicious input.")]),t._v(" "),e("p",[t._v('Least-to-most,Breaking a complex problem down into smaller sub-problems.,"This is a reasoning strategy. An attacker could still inject a ""sub-problem"" that hijacks the model."')]),t._v(" "),e("p",[t._v('Chain-of-thought,Encouraging the model to show its step-by-step reasoning.,"While it improves logic, it can actually make injections easier to see, but it doesn\'t stop the model from following a malicious instruction."')])])}),[],!1,null,null,null);e.default=n.exports}}]);