(window.webpackJsonp=window.webpackJsonp||[]).push([[101],{398:function(e,t,r){"use strict";r.r(t);var s=r(14),o=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("A company has fine-tuned a large language model （LLM） to answer questions\nfor a help desk. The company wants to determine if the fine-tuning has enhanced the\nmodel's accuracy. Which metric should the company use for the evaluation?\nA.Precision\nB.Time to first token\nC.F1 score\nD.Word error rate")]),e._v(" "),t("p",[e._v("The correct answer is C. F1 score.")]),e._v(" "),t("p",[e._v("In the context of evaluating large language models (LLMs) for tasks like a help desk (Question Answering), the F1 score is a standard metric used to measure accuracy. It provides a balanced assessment of how well the model's response matches the ground-truth answer.")]),e._v(" "),t("p",[e._v('Why F1 Score is the Best Choice\nFor text-based tasks, "accuracy" isn\'t a simple "yes/no" calculation. The F1 score is used because it combines two critical factors:')]),e._v(" "),t("p",[e._v("Precision: How much of the model's generated answer is relevant and correct?")]),e._v(" "),t("p",[e._v("Recall: Did the model capture all the important information from the reference answer?")]),e._v(" "),t("p",[e._v("By taking the harmonic mean of these two, the F1 score rewards models that are both concise (high precision) and comprehensive (high recall), which is ideal for a help desk environment where users need both correct and complete information.")]),e._v(" "),t("p",[e._v("Why the other options are incorrect:")]),e._v(" "),t("p",[e._v("Metric,Primary Use Case,Why it's not the best for LLM accuracy")]),e._v(" "),t("p",[e._v('Precision,Classification / Retrieval,"Precision alone ignores ""Recall."" A model could give a very short, one-word correct answer and have 100% precision, but fail to actually help the user (low recall)."')]),e._v(" "),t("p",[e._v('Time to first token,Performance / Latency,"This measures speed, not correctness. It tells you how fast the model starts responding, which is irrelevant to whether the answer is accurate."')]),e._v(" "),t("p",[e._v('Word error rate,Speech-to-Text (ASR),"This measures the number of insertions, deletions, or substitutions. It is used to see how well a transcript matches audio, not how well an AI understands and answers a question."')]),e._v(" "),t("p",[e._v("Common Evaluation Pair")]),e._v(" "),t("p",[e._v('In professional machine learning evaluations (such as the SQuAD benchmark), the F1 score is almost always paired with Exact Match (EM). While EM requires the model\'s answer to be character-perfect, the F1 score is preferred in help desk scenarios because it is more "forgiving"—it recognizes when a model provides the right information even if the wording or phrasing differs slightly from the reference.')])])}),[],!1,null,null,null);t.default=o.exports}}]);