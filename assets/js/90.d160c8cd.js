(window.webpackJsonp=window.webpackJsonp||[]).push([[90],{389:function(e,o,t){"use strict";t.r(o);var n=t(14),s=Object(n.a)({},(function(){var e=this,o=e._self._c;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("p",[e._v("A company is using few-shot prompting on a base model that is hosted on\nAmazon Bedrock. The model currently uses 10 examples in the prompt.The model is\ninvoked once daily and is performing well. The company wants to lower the\nmonthly cost. Which solution will meet these requirements?\nA.Customize the model by using fine-tuning.\nB.Decrease the number of tokens in the prompt.\nC.Increase the number of tokens in the prompt.\nD.Use Provisioned Throughput.")]),e._v(" "),o("p",[e._v("The correct answer is B. Decrease the number of tokens in the prompt.")]),e._v(" "),o("p",[e._v("Reason for this answer:")]),e._v(" "),o("p",[e._v("To understand why B is the correct solution, we have to look at how Amazon Bedrock pricing works and the specific usage pattern mentioned in the prompt (invoked once daily).")]),e._v(" "),o("ol",[o("li",[e._v("Amazon Bedrock Pricing Structure\nAmazon Bedrock primarily uses On-Demand pricing, where you are charged based on the number of input tokens (the prompt you send) and output tokens (the response the model generates).")])]),e._v(" "),o("p",[e._v('Input tokens = Cost: Every example provided in a "few-shot" prompt adds to the input token count.\nThe Current Scenario: The model uses 10 examples. If each example is 100 tokens, that\'s 1,000 tokens per invocation just for the examples.\nThe Solution: By decreasing the number of examples (e.g., from 10 to 3) or making the prompt more concise, you directly reduce the number of input tokens, thereby reducing the cost of that daily invocation.')]),e._v(" "),o("ol",{attrs:{start:"2"}},[o("li",[e._v('Why "Once Daily" matters\nThe frequency of use is the most important clue in this question. Because the model is only used once a day, the total monthly cost is extremely low under On-Demand pricing.')])]),e._v(" "),o("p",[e._v('Why D (Provisioned Throughput) is wrong: Provisioned Throughput is designed for large-scale, consistent workloads. It requires you to pay for a "unit" of capacity per hour (often with a 1-month or 6-month commitment). Paying for a model to be "active" 24/7 just to use it for a few seconds once a day would be massively more expensive than paying for tokens on demand.\nWhy A (Fine-tuning) is wrong: In Amazon Bedrock, if you fine-tune a model, you must purchase Provisioned Throughput to actually use that custom model. As explained above, Provisioned Throughput is cost-prohibitive for a once-per-day task.')]),e._v(" "),o("ol",{attrs:{start:"3"}},[o("li",[e._v("Why C is wrong\nIncreasing the number of tokens would increase the input count, which directly increases the cost under the On-Demand billing model.")])]),e._v(" "),o("p",[e._v("Summary\nSince the model is already performing well and is only used once a day, the most effective way to shave off costs is to optimize the prompt (few-shot engineering) to use fewer tokens, as you only pay for what you send to the model.")])])}),[],!1,null,null,null);o.default=s.exports}}]);