(window.webpackJsonp=window.webpackJsonp||[]).push([[92],{381:function(e,n,i){"use strict";i.r(n);var t=i(14),a=Object(t.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("Which option is a benefit of ongoing pre-training when fine-tuning a foundation\nmodel （FM）?\nA.Helps decrease the model's complexity\nB.Improves model performance over time\nC.Decreases the training time requirement\nD.Optimizes model inference time")]),e._v(" "),n("p",[e._v("The correct answer is B. Improves model performance over time.")]),e._v(" "),n("p",[e._v("Explanation:")]),e._v(" "),n("p",[e._v("Ongoing pre-training (also known as continuous pre-training or domain-adaptive pre-training) is the process of taking a base Foundation Model (FM) that has already been trained on a massive general dataset and continuing the pre-training phase using a more specific or updated dataset.")]),e._v(" "),n("p",[e._v("Here is why B is the correct choice:")]),e._v(" "),n("p",[e._v('Domain Adaptation: General foundation models are jacks-of-all-trades. If you are working in a specialized field (like medical, legal, or financial), ongoing pre-training allows the model to learn the specific vocabulary, jargon, and nuances of that industry, which significantly boosts its performance on related tasks.\nKeeping Knowledge Current: Foundation models are "frozen" in time based on their training data cutoff. Ongoing pre-training allows you to feed the model newer data, ensuring its internal knowledge remains relevant and accurate as the world changes.\nBetter Initialization for Fine-Tuning: By doing ongoing pre-training on domain-specific data first, the model starts from a much better "baseline" before it undergoes task-specific fine-tuning (like sentiment analysis or summarization).')]),e._v(" "),n("p",[e._v("Why the other options are incorrect:")]),e._v(" "),n("p",[e._v("A. Helps decrease the model's complexity: Ongoing pre-training does not change the architecture (the number of layers or parameters) of the model. Therefore, the complexity remains the same.\nC. Decreases the training time requirement: In fact, ongoing pre-training increases the total training time because you are adding an extra step between the initial base model and the final fine-tuned model.\nD. Optimizes model inference time: Inference time (how fast the model generates a response) is determined by the model's size and the hardware it runs on. Training a model on more data does not make it run faster during deployment.")])])}),[],!1,null,null,null);n.default=a.exports}}]);