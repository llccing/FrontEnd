(window.webpackJsonp=window.webpackJsonp||[]).push([[90],{373:function(e,a,t){"use strict";t.r(a);var n=t(14),s=Object(n.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("A company has built a solution by using generative Al. The solution uses large\nlanguage models （LLMs） to translate training manuals from English into other\nlanguages. The company wants to evaluate the accuracy of the solution by\nexamining the text generated for the manuals. Which model evaluation strategy\nmeets these requirements?\nA.Bilingual Evaluation Understudy （BLEU）\nB.Root mean squared error （RMSE）\nC.Recall-Oriented Understudy for Gisting Evaluation （ROUGE）\nD.F1 score")]),e._v(" "),a("p",[e._v("==================================\nThe correct answer is A. Bilingual Evaluation Understudy (BLEU).")]),e._v(" "),a("p",[e._v("Reason for this choice:")]),e._v(" "),a("ol",[a("li",[e._v('What is BLEU?\nBLEU is the industry-standard metric specifically designed for Machine Translation (MT). It measures the similarity between a machine-generated translation and a high-quality "reference" translation produced by a human. It calculates a score based on the overlap of words and phrases (n-grams) between the two texts.')])]),e._v(" "),a("p",[e._v("Since the company's goal is to evaluate the translation of training manuals from English to other languages, BLEU is the most appropriate tool for the job.")]),e._v(" "),a("p",[e._v("Why the other options are incorrect:")]),e._v(" "),a("p",[e._v("B. Root mean squared error (RMSE):\nThis is a mathematical metric used in regression analysis to measure the difference between predicted numerical values and actual numerical values (e.g., predicting house prices or stock market trends). It cannot be used to evaluate the linguistic quality of text.")]),e._v(" "),a("p",[e._v('C. Recall-Oriented Understudy for Gisting Evaluation (ROUGE):\nWhile ROUGE is similar to BLEU, it is primarily used for text summarization. It measures how much of the reference text is "captured" or recalled by the generated summary. While it can technically be used for translation, BLEU is the specific standard for translation tasks.')]),e._v(" "),a("p",[e._v('D. F1 score:\nThe F1 score is a metric used for classification tasks (e.g., "Is this email spam or not?"). It balances precision and recall. While there are versions of F1 scores used in NLP, it is not a standard "strategy" for evaluating the overall fluency and accuracy of a translated manual.')]),e._v(" "),a("p",[e._v("Summary\nFor Machine Translation accuracy, BLEU is always the primary metric to look for.")])])}),[],!1,null,null,null);a.default=s.exports}}]);