(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{325:function(e,t,a){"use strict";a.r(t);var n=a(14),o=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"amazon-bedrock"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#amazon-bedrock"}},[e._v("#")]),e._v(" Amazon Bedrock")]),e._v(" "),t("p",[e._v("The platform for building generative AI applications and agents at production scale")]),e._v(" "),t("h2",{attrs:{id:"bedrock-knowledge-base"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bedrock-knowledge-base"}},[e._v("#")]),e._v(" Bedrock knowledge base")]),e._v(" "),t("p",[e._v("aws link https://aws.amazon.com/bedrock/knowledge-bases/")]),e._v(" "),t("p",[e._v("Amazon Bedrock Knowledge Bases are a powerful feature within Amazon Bedrock that allow "),t("strong",[e._v("Large Language Models (LLMs) to answer questions and complete tasks using your own proprietary data")]),e._v(".")]),e._v(" "),t("p",[e._v("Without a Knowledge Base, LLMs in Bedrock (or any LLM) rely on the data they were initially trained on. This is excellent for general knowledge, creative writing, and common tasks. However, it's useless for questions like:")]),e._v(" "),t("ul",[t("li",[e._v('"What are the vacation policies for engineers at our company?"')]),e._v(" "),t("li",[e._v('"Summarize the key findings from our latest internal market research report."')]),e._v(" "),t("li",[e._v('"Provide steps for troubleshooting issue X based on our product manual."')])]),e._v(" "),t("p",[e._v("This is where Knowledge Bases come in. They bridging the gap between an LLM's general intelligence and your specific, up-to-date, and private information.")]),e._v(" "),t("p",[t("strong",[e._v("How Bedrock Knowledge Bases Work (High-Level):")])]),e._v(" "),t("ol",[t("li",[t("strong",[e._v("Data Ingestion:")]),e._v(" "),t("ul",[t("li",[e._v("You point Bedrock to your data sources, typically stored in Amazon S3 buckets. This data can be in various formats like PDFs, "),t("code",[e._v(".txt")]),e._v(" files, "),t("code",[e._v(".csv")]),e._v(" files, "),t("code",[e._v(".docx")]),e._v(" files, "),t("code",[e._v(".html")]),e._v(" files, etc.")]),e._v(" "),t("li",[e._v("Bedrock automatically processes this data. It splits the documents into smaller, manageable chunks (to fit within the LLM's context window).")]),e._v(" "),t("li",[e._v("It then creates vector embeddings for each chunk of data. These embeddings are numerical representations that capture the semantic meaning of the text.")])])]),e._v(" "),t("li",[t("strong",[e._v("Vector Store (e.g., Pinecone, Redis Enterprise Cloud, OpenSearch Serverless, Aurora):")]),e._v(" "),t("ul",[t("li",[e._v("These vector embeddings are stored in a specialized database called a vector store (or vector database). You configure Bedrock to use a supported vector store.")]),e._v(" "),t("li",[e._v('This vector store allows for efficient "semantic search" – finding chunks of text that are '),t("em",[e._v("similar in meaning")]),e._v(" to a user's query, even if they don't share exact keywords.")])])]),e._v(" "),t("li",[t("strong",[e._v("Retrieve-and-Generate (RAG) Process:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("User Query:")]),e._v(" A user asks a question to an LLM powered by a Knowledge Base.")]),e._v(" "),t("li",[t("strong",[e._v("Retrieval:")]),e._v(" Bedrock first takes the user's query and converts it into a vector embedding. It then performs a semantic search "),t("em",[e._v("against your vector store")]),e._v(" to find the most relevant chunks of proprietary data.")]),e._v(" "),t("li",[t("strong",[e._v("Augmentation:")]),e._v(' These relevant data chunks (the "context") are then passed '),t("em",[e._v("along with the original user query")]),e._v(" to the chosen LLM in Bedrock.")]),e._v(" "),t("li",[t("strong",[e._v("Generation:")]),e._v(" The LLM uses this provided context to generate a precise and accurate answer that is grounded in your data, rather than just its general training data.")])])])]),e._v(" "),t("p",[t("strong",[e._v("Key Benefits of Bedrock Knowledge Bases:")])]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Grounding in Proprietary Data:")]),e._v(" LLMs can provide accurate, up-to-date, and specific answers based on "),t("em",[e._v("your")]),e._v(" internal documents, manuals, reports, and knowledge bases.")]),e._v(" "),t("li",[t("strong",[e._v("Reduced Hallucinations:")]),e._v(' By providing relevant context, Knowledge Bases significantly reduce the LLM\'s tendency to "hallucinate" or make up information, leading to more reliable outputs.')]),e._v(" "),t("li",[t("strong",[e._v("Always Up-to-Date:")]),e._v(" As you update your source data in S3, you can re-sync your Knowledge Base, ensuring the LLM always has access to the latest information.")]),e._v(" "),t("li",[t("strong",[e._v("Simplified RAG Implementation:")]),e._v(" Bedrock handles much of the heavy lifting for implementing the RAG (Retrieve-and-Generate) pattern, including data chunking, embedding generation, and orchestrating the retrieval process, making it much easier for developers.")]),e._v(" "),t("li",[t("strong",[e._v("Security and Privacy:")]),e._v(" Your data remains within your AWS account and is not used to train the underlying foundation models.")]),e._v(" "),t("li",[t("strong",[e._v("Improved User Experience:")]),e._v(" Users get direct, relevant answers from your specific domain, rather than vague or general responses.")]),e._v(" "),t("li",[t("strong",[e._v("Self-Service for Information:")]),e._v(" Empower employees or customers to find answers themselves, reducing reliance on support staff.")])]),e._v(" "),t("p",[t("strong",[e._v("Use Cases:")])]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Customer Service Chatbots:")]),e._v(" Provide instant, accurate answers about products, services, and policies using internal documentation.")]),e._v(" "),t("li",[t("strong",[e._v("Internal Knowledge Management:")]),e._v(" Allow employees to quickly find information on company policies, HR benefits, IT troubleshooting, project details, etc.")]),e._v(" "),t("li",[t("strong",[e._v("Research and Development:")]),e._v(" Summarize complex research papers or internal reports, extract specific data points.")]),e._v(" "),t("li",[t("strong",[e._v("Legal Compliance:")]),e._v(" Answer questions about regulatory documents and compliance guidelines.")]),e._v(" "),t("li",[t("strong",[e._v("Content Generation:")]),e._v(" Assist in generating content that needs to be factual and grounded in specific source materials.")])]),e._v(" "),t("p",[e._v("In essence, Bedrock Knowledge Bases transform generic LLMs into powerful, domain-specific experts tailored to your organization's unique information. They are a cornerstone for building truly intelligent AI applications that leverage your valuable enterprise data.")]),e._v(" "),t("h3",{attrs:{id:"the-difference-between-bedrock-and-sagemaker"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-difference-between-bedrock-and-sagemaker"}},[e._v("#")]),e._v(" the difference between bedrock and sagemaker?")]),e._v(" "),t("p",[e._v("AWS Bedrock and Amazon SageMaker both help you build with AI/ML, but they’re aimed at different layers of the stack:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("What they are\nAWS Bedrock: A managed Generative AI service to access and use foundation models (FMs) (LLMs, image models, etc.) via APIs. You primarily consume models (from AWS and partners) and optionally customize them.\nAmazon SageMaker: A full ML platform to build, train, tune, deploy, and operate ML models (including generative AI if you want), with deep control over the ML lifecycle and infrastructure.")])]),e._v(" "),t("li",[t("p",[e._v("Primary use cases\nBedrock is best when you want to:\nQuickly add chat, summarization, extraction, Q&A, agents, and other GenAI features.\nUse managed model endpoints without managing GPUs or model hosting.\nImplement RAG (Retrieval-Augmented Generation) with tools like Knowledge Bases for Bedrock.\nSageMaker is best when you want to:\nTrain your own models (classic ML or deep learning) from data.\nFine-tune large models with custom pipelines, experiments, MLOps.\nRun custom containers, custom inference, specialized hardware, optimization.")])]),e._v(" "),t("li",[t("p",[e._v("Level of control vs speed\nBedrock: Faster to start, less control. You pick a model, prompt it, add guardrails, add RAG, maybe fine-tune (where supported).\nSageMaker: More setup, more control. You choose frameworks, training jobs, hyperparameter tuning, deployment patterns, autoscaling, monitoring, etc.")])]),e._v(" "),t("li",[t("p",[e._v("Hosting and infrastructure\nBedrock: AWS handles model hosting; you pay per usage (tokens, images, etc.). You don’t manage endpoints/instances in the same way.\nSageMaker: You typically deploy models to SageMaker endpoints (real-time, async, batch), choose instance types, scaling policies, and manage costs accordingly.")])]),e._v(" "),t("li",[t("p",[e._v("Customization options\nBedrock: Model-dependent customization (e.g., fine-tuning/continued training where available), plus strong emphasis on prompting + RAG.\nSageMaker: Broadest customization—train from scratch, fine-tune any model you can run, bring your own model/container, build end-to-end ML pipelines.")])]),e._v(" "),t("li",[t("p",[e._v("Governance, safety, and enterprise features\nBedrock: Includes GenAI-focused features like Guardrails, model choice, and built-in tooling for typical GenAI patterns.\nSageMaker: Strong on MLOps (pipelines, model registry, monitoring, lineage), broader ML governance across the lifecycle.")])]),e._v(" "),t("li",[t("p",[e._v("Simple rule of thumb\nChoose Bedrock if you want to integrate GenAI fast using managed foundation models and standard GenAI patterns.\nChoose SageMaker if you need to build/own the ML lifecycle, train/fine-tune with maximum flexibility, or run highly customized inference.")])])]),e._v(" "),t("p",[e._v("If you tell me your target scenario (e.g., chatbot for customer service, automated email/ticket summarization, forecasting, anomaly detection), I can recommend which one fits better and a typical architecture.")]),e._v(" "),t("h2",{attrs:{id:"partyrock"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#partyrock"}},[e._v("#")]),e._v(" PartyRock")]),e._v(" "),t("p",[e._v("an Amazon Bedrock Playground")]),e._v(" "),t("h2",{attrs:{id:"amazon-bedrock-guardrails"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#amazon-bedrock-guardrails"}},[e._v("#")]),e._v(" Amazon Bedrock Guardrails")]),e._v(" "),t("p",[e._v("Implement safeguards customized to your application requirements and responsible AI policies")]),e._v(" "),t("p",[e._v("Amazon Bedrock Guardrails helps you safely build and deploy responsible generative AI applications with confidence. With industry-leading safety protections that block up to 88% of harmful content and deliver auditable, mathematically verifiable explanations for validation decisions with 99% accuracy, Guardrails provides configurable safeguards to help detect and filter harmful text and image content, redact sensitive information, detect model hallucinations, and more. Guardrails work consistently across any foundation model whether you're using models in Amazon Bedrock or self-hosted models including third-party models such as OpenAI and Google Gemini — giving you the same safety, privacy, and responsible AI controls across all your generative AI applications.")]),e._v(" "),t("h2",{attrs:{id:"invocation-logging"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#invocation-logging"}},[e._v("#")]),e._v(" invocation logging")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://builder.aws.com/content/2s5cG1VQe1478chNNsZBYUalvIW/aws-bedrock-model-invocation-logging-an-overview",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://builder.aws.com/content/2s5cG1VQe1478chNNsZBYUalvIW/aws-bedrock-model-invocation-logging-an-overview"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("AWS Bedrock model invocation logging is a useful component of an evaluation-in-depth strategy that provides generative AI developers with important signals about how their applications and selected models perform. For account administrators, it helps detect potentially uncontrolled access to models, as on-demand inference calls are also logged.")])])}),[],!1,null,null,null);t.default=o.exports}}]);