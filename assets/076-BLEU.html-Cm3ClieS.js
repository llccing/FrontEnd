import{_ as a,c as t,a as i,o as r}from"./app-Dbw06csz.js";const o={};function s(n,e){return r(),t("div",null,[...e[0]||(e[0]=[i("<p>A company has built a solution by using generative Al. The solution uses large language models （LLMs） to translate training manuals from English into other languages. The company wants to evaluate the accuracy of the solution by examining the text generated for the manuals. Which model evaluation strategy meets these requirements? A.Bilingual Evaluation Understudy （BLEU） B.Root mean squared error （RMSE） C.Recall-Oriented Understudy for Gisting Evaluation （ROUGE） D.F1 score</p><p>================================== The correct answer is A. Bilingual Evaluation Understudy (BLEU).</p><p>Reason for this choice:</p><ol><li>What is BLEU? BLEU is the industry-standard metric specifically designed for Machine Translation (MT). It measures the similarity between a machine-generated translation and a high-quality &quot;reference&quot; translation produced by a human. It calculates a score based on the overlap of words and phrases (n-grams) between the two texts.</li></ol><p>Since the company&#39;s goal is to evaluate the translation of training manuals from English to other languages, BLEU is the most appropriate tool for the job.</p><p>Why the other options are incorrect:</p><p>B. Root mean squared error (RMSE): This is a mathematical metric used in regression analysis to measure the difference between predicted numerical values and actual numerical values (e.g., predicting house prices or stock market trends). It cannot be used to evaluate the linguistic quality of text.</p><p>C. Recall-Oriented Understudy for Gisting Evaluation (ROUGE): While ROUGE is similar to BLEU, it is primarily used for text summarization. It measures how much of the reference text is &quot;captured&quot; or recalled by the generated summary. While it can technically be used for translation, BLEU is the specific standard for translation tasks.</p><p>D. F1 score: The F1 score is a metric used for classification tasks (e.g., &quot;Is this email spam or not?&quot;). It balances precision and recall. While there are versions of F1 scores used in NLP, it is not a standard &quot;strategy&quot; for evaluating the overall fluency and accuracy of a translated manual.</p><p>Summary For Machine Translation accuracy, BLEU is always the primary metric to look for.</p>",10)])])}const c=a(o,[["render",s]]),u=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/076-BLEU.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/076-BLEU.md"}');export{c as comp,u as data};
