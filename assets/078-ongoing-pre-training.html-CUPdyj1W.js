import{_ as i,c as a,b as e,o as t}from"./app-Dbw06csz.js";const o={};function r(s,n){return t(),a("div",null,[...n[0]||(n[0]=[e("p",null,"Which option is a benefit of ongoing pre-training when fine-tuning a foundation model （FM）? A.Helps decrease the model's complexity B.Improves model performance over time C.Decreases the training time requirement D.Optimizes model inference time",-1),e("p",null,"The correct answer is B. Improves model performance over time.",-1),e("p",null,"Explanation:",-1),e("p",null,"Ongoing pre-training (also known as continuous pre-training or domain-adaptive pre-training) is the process of taking a base Foundation Model (FM) that has already been trained on a massive general dataset and continuing the pre-training phase using a more specific or updated dataset.",-1),e("p",null,"Here is why B is the correct choice:",-1),e("p",null,'Domain Adaptation: General foundation models are jacks-of-all-trades. If you are working in a specialized field (like medical, legal, or financial), ongoing pre-training allows the model to learn the specific vocabulary, jargon, and nuances of that industry, which significantly boosts its performance on related tasks. Keeping Knowledge Current: Foundation models are "frozen" in time based on their training data cutoff. Ongoing pre-training allows you to feed the model newer data, ensuring its internal knowledge remains relevant and accurate as the world changes. Better Initialization for Fine-Tuning: By doing ongoing pre-training on domain-specific data first, the model starts from a much better "baseline" before it undergoes task-specific fine-tuning (like sentiment analysis or summarization).',-1),e("p",null,"Why the other options are incorrect:",-1),e("p",null,"A. Helps decrease the model's complexity: Ongoing pre-training does not change the architecture (the number of layers or parameters) of the model. Therefore, the complexity remains the same. C. Decreases the training time requirement: In fact, ongoing pre-training increases the total training time because you are adding an extra step between the initial base model and the final fine-tuned model. D. Optimizes model inference time: Inference time (how fast the model generates a response) is determined by the model's size and the hardware it runs on. Training a model on more data does not make it run faster during deployment.",-1)])])}const m=i(o,[["render",r]]),c=JSON.parse('{"path":"/AWS/CPE/03Trick-Questions.md/078-ongoing-pre-training.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1768212680000,"contributors":[{"name":"Rowan Liu","username":"","email":"lcf33123@gmail.com","commits":1}],"changelog":[{"hash":"8dc20e2f1b4b500b4a8b691792c2c9b3c12addca","time":1768212680000,"email":"lcf33123@gmail.com","author":"Rowan Liu","message":"Merge pull request #66 from llccing/copilot/fix-vite-build-error"}]},"filePathRelative":"AWS/CPE/03Trick-Questions.md/078-ongoing-pre-training.md"}');export{m as comp,c as data};
